{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7EoXj8ZHGDgSaO9wRMxu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harinijuluru/Explainable-AI/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-pUzuZm-kJPT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "da096019-e63f-4a3f-8438-06769ca4ae76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: ['name', 'full_name', 'birth_date', 'age', 'height_cm', 'weight_kgs', 'positions', 'nationality', 'overall_rating', 'potential', 'value_euro', 'wage_euro', 'preferred_foot', 'international_reputation(1-5)', 'weak_foot(1-5)', 'skill_moves(1-5)', 'body_type', 'release_clause_euro', 'national_team', 'national_rating', 'national_team_position', 'national_jersey_number', 'crossing', 'finishing', 'heading_accuracy', 'short_passing', 'volleys', 'dribbling', 'curve', 'freekick_accuracy', 'long_passing', 'ball_control', 'acceleration', 'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power', 'jumping', 'stamina', 'strength', 'long_shots', 'aggression', 'interceptions', 'positioning', 'vision', 'penalties', 'composure', 'marking', 'standing_tackle', 'sliding_tackle']\n",
            "Using target column: overall_rating\n",
            "[LightGBM] [Info] Number of positive: 6867, number of negative: 5700\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147667 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3473\n",
            "[LightGBM] [Info] Number of data points in the train set: 12567, number of used features: 196\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546431 -> initscore=0.186261\n",
            "[LightGBM] [Info] Start training from score 0.186261\n",
            "\n",
            "Model Evaluation:\n",
            "Accuracy: 0.9922034527566364\n",
            "Precision: 0.9948840381991815\n",
            "Recall: 0.990828804347826\n",
            "F1-score: 0.9928522804629\n",
            "ROC AUC: 0.992344406267241\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      2443\n",
            "           1       0.99      0.99      0.99      2944\n",
            "\n",
            "    accuracy                           0.99      5387\n",
            "   macro avg       0.99      0.99      0.99      5387\n",
            "weighted avg       0.99      0.99      0.99      5387\n",
            "\n",
            "\n",
            "Top features (model importance):\n",
            "           feature  importance\n",
            "4       value_euro        1112\n",
            "3        potential         782\n",
            "0              age         697\n",
            "12        crossing         180\n",
            "17       dribbling         170\n",
            "40  sliding_tackle         169\n",
            "33   interceptions         159\n",
            "25       reactions         153\n",
            "34     positioning         141\n",
            "29         stamina         129\n",
            "\n",
            "✅ Done. Plots saved to ./outputs :\n",
            " - shap_summary_beeswarm.png\n",
            " - shap_summary_bar.png\n",
            " - shap_waterfall.png\n",
            " - shap_force.png\n",
            "And (optional) model_feature_importance_top20.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ===========================================\n",
        "# SHAP Feature Importance – FIFA Players (Binary Classification)\n",
        "# Fast + Version-Safe + Robust to Column Names\n",
        "# ===========================================\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import sklearn\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, classification_report\n",
        ")\n",
        "\n",
        "# If LightGBM isn't available, fall back to RandomForest\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    USE_LGBM = True\n",
        "except Exception:\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    USE_LGBM = False\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load Dataset\n",
        "# -----------------------------\n",
        "CSV_PATH = \"fifa_players.csv\"  # put the CSV beside this script/notebook\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Columns in dataset:\", df.columns.tolist())\n",
        "\n",
        "# Basic cleaning\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Pick Target Column (flexible)\n",
        "# -----------------------------\n",
        "# Prefer well-known rating/value columns if present\n",
        "preferred_targets = [\n",
        "    \"overall_rating\", \"overall\", \"national_rating\",\n",
        "    \"value_euro\", \"wage_euro\", \"potential\"\n",
        "]\n",
        "\n",
        "target_col = None\n",
        "for c in preferred_targets:\n",
        "    if c in df.columns:\n",
        "        target_col = c\n",
        "        break\n",
        "\n",
        "# Fallback: last numeric column\n",
        "if target_col is None:\n",
        "    numeric_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    assert len(numeric_cols_all) > 0, \"No numeric columns found to use as a target.\"\n",
        "    target_col = numeric_cols_all[-1]\n",
        "\n",
        "print(\"Using target column:\", target_col)\n",
        "\n",
        "# Binary target via median split (High vs Low)\n",
        "df = df.dropna(subset=[target_col]).copy()\n",
        "threshold = df[target_col].median()\n",
        "df[\"Target\"] = (df[target_col] >= threshold).astype(int)\n",
        "\n",
        "# Features/labels\n",
        "X = df.drop(columns=[\"Target\", target_col])\n",
        "y = df[\"Target\"]\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Train/Test Split\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Preprocessing\n",
        "# -----------------------------\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Version-safe OneHotEncoder: older sklearn uses \"sparse\", newer has \"sparse_output\"\n",
        "try:\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
        "    ])\n",
        "except TypeError:\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
        "    ])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Model (fast defaults)\n",
        "# -----------------------------\n",
        "if USE_LGBM:\n",
        "    model = LGBMClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=-1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "else:\n",
        "    # Fallback: still good & fast\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=12,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", model)\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Train\n",
        "# -----------------------------\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Evaluation\n",
        "# -----------------------------\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Feature Names after Preprocessing\n",
        "# -----------------------------\n",
        "# Fit preprocessor alone (already fitted inside pipeline, but we need the encoder to pull names)\n",
        "pre = clf.named_steps[\"preprocessor\"]\n",
        "# Access the fitted OneHotEncoder inside ColumnTransformer\n",
        "ohe = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"] if len(categorical_features) else None\n",
        "if ohe is not None and hasattr(ohe, \"get_feature_names_out\"):\n",
        "    cat_out = ohe.get_feature_names_out(categorical_features)\n",
        "else:\n",
        "    cat_out = np.array([])\n",
        "\n",
        "feature_names_transformed = list(numeric_features) + list(cat_out)\n",
        "\n",
        "# -----------------------------\n",
        "# 9) Transform Test Set (for SHAP)\n",
        "# -----------------------------\n",
        "# Will be a sparse matrix if OHE is sparse; convert a small sample to dense for SHAP plots\n",
        "X_test_trans = pre.transform(X_test)  # sparse or dense\n",
        "# Sample up to 300 rows for speed\n",
        "sample_size = min(300, X_test_trans.shape[0])\n",
        "if hasattr(X_test_trans, \"toarray\"):\n",
        "    X_sample = X_test_trans[:sample_size].toarray()\n",
        "else:\n",
        "    X_sample = np.asarray(X_test_trans[:sample_size])\n",
        "\n",
        "# -----------------------------\n",
        "# 10) SHAP Explainer (TreeExplainer)\n",
        "# -----------------------------\n",
        "# For LightGBM: use native TreeExplainer; for RF fallback it's also supported\n",
        "est = clf.named_steps[\"model\"]\n",
        "explainer = shap.TreeExplainer(est, feature_perturbation=\"tree_path_dependent\")\n",
        "\n",
        "# LightGBM binary returns a list [class0, class1]; RandomForest often returns numpy for binary, but handle both\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "if isinstance(shap_values, list):\n",
        "    # Take positive class contributions\n",
        "    shap_vals_pos = shap_values[1]\n",
        "else:\n",
        "    shap_vals_pos = shap_values\n",
        "\n",
        "# Align shapes just in case (#features can differ if encoder pruned unseen categories)\n",
        "min_len = min(shap_vals_pos.shape[1], len(feature_names_transformed))\n",
        "shap_vals_pos = shap_vals_pos[:, :min_len]\n",
        "feature_names_aligned = feature_names_transformed[:min_len]\n",
        "X_sample_aligned = X_sample[:, :min_len]\n",
        "\n",
        "# -----------------------------\n",
        "# 11) Save Plots (Summary Beeswarm + Bar)\n",
        "# -----------------------------\n",
        "outdir = Path(\"outputs\")\n",
        "outdir.mkdir(exist_ok=True)\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_vals_pos, X_sample_aligned,\n",
        "                  feature_names=feature_names_aligned, show=False)\n",
        "plt.title(\"SHAP Summary (Beeswarm) – Sampled\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(outdir / \"shap_summary_beeswarm.png\", bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_vals_pos, X_sample_aligned,\n",
        "                  feature_names=feature_names_aligned, plot_type=\"bar\", show=False)\n",
        "plt.title(\"SHAP Feature Importance (Bar) – Sampled\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(outdir / \"shap_summary_bar.png\", bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# 12) Optional: Force & Waterfall for one instance (fast-safe)\n",
        "# -----------------------------\n",
        "# Use legacy waterfall to avoid Explanation API mismatches across SHAP versions\n",
        "idx = 0\n",
        "base_value = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
        "\n",
        "plt.figure()\n",
        "shap.plots._waterfall.waterfall_legacy(\n",
        "    base_value, shap_vals_pos[idx], feature_names=feature_names_aligned, show=False, max_display=20\n",
        ")\n",
        "plt.title(\"SHAP Waterfall – One Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(outdir / \"shap_waterfall.png\", bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "shap.force_plot(base_value, shap_vals_pos[idx, :], X_sample_aligned[idx, :],\n",
        "                feature_names=feature_names_aligned, matplotlib=True, show=False)\n",
        "plt.title(\"SHAP Force Plot – One Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(outdir / \"shap_force.png\", bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# 13) (Optional) Model Feature Importance Table\n",
        "# -----------------------------\n",
        "try:\n",
        "    importances = est.feature_importances_\n",
        "    fi = (pd.DataFrame({\n",
        "        \"feature\": feature_names_aligned,\n",
        "        \"importance\": importances[:len(feature_names_aligned)]\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .head(20))\n",
        "    fi.to_csv(outdir / \"/content/fifa_players.csv\", index=False)\n",
        "    print(\"\\nTop features (model importance):\")\n",
        "    print(fi.head(10))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(\"\\n✅ Done. Plots saved to ./outputs :\")\n",
        "print(\" - shap_summary_beeswarm.png\")\n",
        "print(\" - shap_summary_bar.png\")\n",
        "print(\" - shap_waterfall.png\")\n",
        "print(\" - shap_force.png\")\n",
        "print(\"And (optional) model_feature_importance_top20.csv\")"
      ]
    }
  ]
}